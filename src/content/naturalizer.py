"""
M4å†…å®¹å·¥å‚ - è‡ªç„¶åŒ–å¤„ç†å™¨
è´Ÿè´£æ£€æŸ¥å’Œä¼˜åŒ–è¯„è®ºçš„è‡ªç„¶åº¦å’Œå¤šæ ·æ€§
"""

import re
import random
import yaml
from pathlib import Path
from typing import List, Dict, Optional
from collections import Counter

from src.content.models import Persona
from src.core.logging import get_logger

logger = get_logger(__name__)


class Naturalizer:
    """
    è‡ªç„¶åŒ–å¤„ç†å™¨
    æ£€æŸ¥å¥å¼å¤šæ ·æ€§ã€å£å¤´ç¦…å¯†åº¦ã€n-gramé‡å¤åº¦
    [FIX 2025-10-10] æ–°å¢ï¼šå£å¤´ç¦…æ³¨å…¥ã€è‡ªç„¶ç‘•ç–µæ·»åŠ ã€å¥å¼å¤šæ ·åŒ–
    """

    def __init__(
        self,
        ngram_window_days: int = 7,
        max_ngram_reuse: float = 0.35,
        naturalization_policy_path: Optional[Path] = None
    ):
        """
        åˆå§‹åŒ–è‡ªç„¶åŒ–å¤„ç†å™¨

        Args:
            ngram_window_days: n-gramå»é‡çª—å£ï¼ˆå¤©ï¼‰
            max_ngram_reuse: æœ€å¤§n-gramé‡å¤ç‡
            naturalization_policy_path: è‡ªç„¶åŒ–ç­–ç•¥é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.ngram_window_days = ngram_window_days
        self.max_ngram_reuse = max_ngram_reuse
        self.ngram_history: List[str] = []  # å­˜å‚¨å†å²8-gram

        # [FIX 2025-10-10] åŠ è½½è‡ªç„¶åŒ–ç­–ç•¥é…ç½®
        self.policy = {}
        if naturalization_policy_path and naturalization_policy_path.exists():
            with open(naturalization_policy_path, 'r', encoding='utf-8') as f:
                self.policy = yaml.safe_load(f)
        else:
            # é»˜è®¤ç­–ç•¥
            self.policy = self._default_policy()

    def process(
        self,
        comment_text: str,
        persona: Persona
    ) -> str:
        """
        å¤„ç†è¯„è®ºæ–‡æœ¬ï¼Œç¡®ä¿è‡ªç„¶åº¦

        [FIX 2025-10-10] æ–°å¢è‡ªåŠ¨å¤„ç†æ­¥éª¤ï¼š
        1. æ³¨å…¥å£å¤´ç¦…ï¼ˆæ ¹æ®Personaï¼‰
        2. æ·»åŠ è‡ªç„¶ç‘•ç–µï¼ˆè¡¨æƒ…ã€é”™å­—ã€å£è¯­è¯ï¼‰
        3. å¥å¼å¤šæ ·åŒ–
        4. æ£€æŸ¥è´¨é‡ï¼ˆå¥å¼ã€å¯†åº¦ã€n-gramï¼‰

        Args:
            comment_text: åŸå§‹è¯„è®ºæ–‡æœ¬
            persona: ä½¿ç”¨çš„Persona

        Returns:
            å¤„ç†åçš„è‡ªç„¶åŒ–è¯„è®ºæ–‡æœ¬
        """
        # [FIX 2025-10-10] æ­¥éª¤1ï¼šæ³¨å…¥å£å¤´ç¦…
        text = self.inject_catchphrases(comment_text, persona)

        # [FIX 2025-10-10] æ­¥éª¤2ï¼šæ·»åŠ è‡ªç„¶ç‘•ç–µ
        text = self.add_natural_imperfections(text)

        # [FIX 2025-10-10] æ­¥éª¤3ï¼šå¥å¼å¤šæ ·åŒ–
        text = self.vary_sentence_structure(text)

        # åŸæœ‰æ£€æŸ¥æµç¨‹
        sentence_issues = self._check_sentence_variety(text)
        if sentence_issues:
            logger.debug("Sentence variety issues detected", issues=sentence_issues)

        catchphrase_issues = self._check_catchphrase_density(
            text,
            persona.catchphrases
        )
        if catchphrase_issues:
            logger.debug("Catchphrase density issues detected", issues=catchphrase_issues)

        ngram_issues = self._check_ngram_uniqueness(text)
        if ngram_issues:
            logger.warning("N-gram repetition detected", issues=ngram_issues)

        self._record_ngrams(text)

        return text

    def _check_sentence_variety(self, text: str) -> List[str]:
        """
        æ£€æŸ¥å¥å¼å¤šæ ·æ€§

        æ£€æŸ¥ï¼š
        - æ˜¯å¦è‡³å°‘åŒ…å«2ç§å¥å¼ï¼ˆé™ˆè¿°/ç–‘é—®/åˆ—è¡¨ï¼‰
        - å¥å¼ç»“æ„é‡å¤ç‡æ˜¯å¦è¶…è¿‡50%
        """
        issues = []

        sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]
        if len(sentences) < 2:
            return issues  # å¤ªçŸ­ï¼Œæ— éœ€æ£€æŸ¥

        # ç»Ÿè®¡å¥å¼ç±»å‹
        statements = sum(1 for s in sentences if not s.endswith('?'))
        questions = sum(1 for s in sentences if s.endswith('?'))

        sentence_types = sum([statements > 0, questions > 0])
        if sentence_types < 2:
            issues.append("Only one sentence type detected")

        # æ£€æŸ¥å¥å­ç»“æ„é‡å¤ï¼ˆç®€åŒ–ï¼šæ£€æŸ¥å¼€å¤´è¯ï¼‰
        opening_words = [s.split()[0].lower() for s in sentences if s.split()]
        if opening_words:
            most_common_count = Counter(opening_words).most_common(1)[0][1]
            repeat_ratio = most_common_count / len(opening_words)
            if repeat_ratio > 0.5:
                issues.append(f"Sentence structure repetition: {repeat_ratio:.0%}")

        return issues

    def _check_catchphrase_density(
        self,
        text: str,
        catchphrases: Dict[str, List[str]]
    ) -> List[str]:
        """
        æ£€æŸ¥å£å¤´ç¦…å¯†åº¦

        ç›®æ ‡ï¼šåŒpersona 7å¤©å†…å£å¤´ç¦…ä¸è¶…è¿‡20%
        """
        issues = []

        all_phrases = []
        for phrases_list in catchphrases.values():
            all_phrases.extend(phrases_list)

        text_lower = text.lower()
        sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]

        catchphrase_count = sum(1 for phrase in all_phrases if phrase.lower() in text_lower)
        if not sentences:
            return issues

        density = catchphrase_count / len(sentences)
        if density > 0.20:
            issues.append(f"Catchphrase density too high: {density:.0%} (max 20%)")

        return issues

    def _check_ngram_uniqueness(self, text: str) -> List[str]:
        """
        æ£€æŸ¥8-gramé‡å¤åº¦

        ä¸å†å²è®°å½•å¯¹æ¯”ï¼Œé‡å¤ç‡ä¸è¶…è¿‡35%
        """
        issues = []

        current_ngrams = self._extract_8grams(text)
        if not current_ngrams:
            return issues

        # ä¸å†å²å¯¹æ¯”
        if self.ngram_history:
            overlap = sum(1 for ng in current_ngrams if ng in self.ngram_history)
            reuse_ratio = overlap / len(current_ngrams)

            if reuse_ratio > self.max_ngram_reuse:
                issues.append(
                    f"8-gram reuse rate: {reuse_ratio:.0%} (max {self.max_ngram_reuse:.0%})"
                )

        return issues

    def _extract_8grams(self, text: str) -> List[str]:
        """
        æå–æ–‡æœ¬ä¸­çš„8-gramï¼ˆè¿ç»­8ä¸ªè¯ï¼‰

        Returns:
            8-gramå­—ç¬¦ä¸²åˆ—è¡¨
        """
        words = text.split()
        if len(words) < 8:
            return []

        ngrams = []
        for i in range(len(words) - 7):
            ngram = ' '.join(words[i:i+8])
            ngrams.append(ngram.lower())

        return ngrams

    def _record_ngrams(self, text: str):
        """è®°å½•å½“å‰è¯„è®ºçš„8-gramåˆ°å†å²"""
        ngrams = self._extract_8grams(text)
        self.ngram_history.extend(ngrams)

        # é™åˆ¶å†å²å¤§å°ï¼ˆä¿ç•™æœ€è¿‘1000ä¸ªï¼‰
        if len(self.ngram_history) > 1000:
            self.ngram_history = self.ngram_history[-1000:]

    def clear_history(self):
        """æ¸…ç©ºn-gramå†å²ï¼ˆæµ‹è¯•/è°ƒè¯•ç”¨ï¼‰"""
        self.ngram_history.clear()
        logger.info("Cleared n-gram history")

    # [FIX 2025-10-10] æ–°å¢æ–¹æ³•

    def inject_catchphrases(self, text: str, persona: Persona) -> str:
        """
        éšæœºæ³¨å…¥Personaå£å¤´ç¦…ï¼ˆå¼€å¤´/è¿‡æ¸¡/ç»“å°¾ï¼‰

        ç­–ç•¥ï¼š
        - 25%æ¦‚ç‡åœ¨å¼€å¤´æ³¨å…¥opening catchphrase
        - 20%æ¦‚ç‡åœ¨ä¸­é—´æ³¨å…¥transition catchphrase
        - 30%æ¦‚ç‡åœ¨ç»“å°¾æ³¨å…¥ending catchphrase
        """
        # [FIX 2025-10-10] ä¿®å¤ï¼šä½¿ç”¨æ›´å¥½çš„å¥å­åˆ†å‰²ï¼Œä¿ç•™æ ‡ç‚¹
        sentences = re.split(r'(?<=[.!?])\s+', text)
        if len(sentences) < 2:
            return text  # å¤ªçŸ­ï¼Œä¸æ³¨å…¥

        # å¼€å¤´æ³¨å…¥ï¼ˆ25%æ¦‚ç‡ï¼‰
        if random.random() < 0.25 and persona.catchphrases.get('opening'):
            opening_phrase = random.choice(persona.catchphrases['opening'])
            # å¦‚æœç¬¬ä¸€å¥ä¸æ˜¯ä»¥å£å¤´ç¦…å¼€å¤´ï¼Œåˆ™æ·»åŠ 
            if not any(sentences[0].lower().startswith(p.lower()) for p in persona.catchphrases.get('opening', [])):
                sentences[0] = f"{opening_phrase} {sentences[0]}"

        # è¿‡æ¸¡æ³¨å…¥ï¼ˆ20%æ¦‚ç‡ï¼Œæ’å…¥ä¸­é—´ï¼‰
        if random.random() < 0.20 and persona.catchphrases.get('transition'):
            if len(sentences) >= 3:  # è‡³å°‘3å¥æ‰æ’å…¥ä¸­é—´
                transition_phrase = random.choice(persona.catchphrases['transition'])
                mid_idx = len(sentences) // 2
                sentences[mid_idx] = f"{transition_phrase} {sentences[mid_idx]}"

        # ç»“å°¾æ³¨å…¥ï¼ˆ30%æ¦‚ç‡ï¼‰
        if random.random() < 0.30 and persona.catchphrases.get('ending'):
            ending_phrase = random.choice(persona.catchphrases['ending'])
            # åœ¨æœ€åæ·»åŠ 
            if not sentences[-1].endswith('.'):
                sentences[-1] += '.'
            sentences[-1] = sentences[-1].rstrip('.') + f'. {ending_phrase}'

        return ' '.join(sentences)

    def add_natural_imperfections(self, text: str) -> str:
        """
        æ·»åŠ è‡ªç„¶ç‘•ç–µï¼ˆè¡¨æƒ…ã€è½»å¾®é”™å­—ã€å£è¯­å¡«å……è¯ï¼‰

        å‚è€ƒnaturalization_policy.yaml:
        - emoji: 25%æ¦‚ç‡ï¼Œæœ€å¤š1ä¸ª
        - typo: 15%æ¦‚ç‡ï¼Œæœ€å¤š1ä¸ª
        - filler_words: 35%æ¦‚ç‡
        """
        # 1. è¡¨æƒ…ç¬¦å·ï¼ˆ25%æ¦‚ç‡ï¼‰
        emoji_policy = self.policy.get('emoji_policy', {})
        if emoji_policy.get('allow', True) and random.random() < emoji_policy.get('probability', 0.25):
            emojis = emoji_policy.get('appropriate_emojis', ['ğŸ‘', 'ğŸ˜‚', 'ğŸ™', 'ğŸ¤”', 'ğŸ‘€'])
            emoji = random.choice(emojis)
            # ä¼˜å…ˆæ”¾åœ¨ç»“å°¾
            text = text.rstrip() + f" {emoji}"

        # 2. è½»å¾®é”™å­—ï¼ˆ15%æ¦‚ç‡ï¼‰
        typo_policy = self.policy.get('typo_policy', {})
        if typo_policy.get('allow_light_typos', True) and random.random() < typo_policy.get('probability', 0.15):
            allowed_typos = typo_policy.get('allowed_typos', [])
            if allowed_typos:
                typo_pair = random.choice(allowed_typos)
                original = typo_pair.get('original', '')
                typo = typo_pair.get('typo', '')
                # åªæ›¿æ¢ä¸€æ¬¡
                text = text.replace(original, typo, 1)

        # 3. å£è¯­å¡«å……è¯ï¼ˆ35%æ¦‚ç‡ï¼‰
        filler_policy = self.policy.get('filler_words', {})
        if filler_policy.get('allow', True) and random.random() < filler_policy.get('probability', 0.35):
            fillers = filler_policy.get('common_fillers', ['tbh', 'imo', 'honestly'])
            filler = random.choice(fillers)
            # æ’å…¥åˆ°ç¬¬ä¸€å¥å¼€å¤´
            sentences = re.split(r'([.!?])', text, maxsplit=1)
            if sentences:
                sentences[0] = f"{filler}, {sentences[0]}"
                text = ''.join(sentences)

        return text

    def vary_sentence_structure(self, text: str) -> str:
        """
        å¥å¼å¤šæ ·åŒ–ï¼ˆè½»é‡å®ç°ï¼‰

        ç­–ç•¥ï¼š
        - æ·»åŠ çœç•¥å·ï¼ˆ20%æ¦‚ç‡ï¼‰
        - è°ƒæ•´æ ‡ç‚¹ï¼ˆé¿å…è¿‡å¤šæ„Ÿå¹å·ï¼‰
        """
        punctuation = self.policy.get('punctuation_variety', {})

        # çœç•¥å·æ›¿æ¢ï¼ˆ20%æ¦‚ç‡ï¼‰
        if punctuation.get('allow_ellipsis', True) and random.random() < punctuation.get('ellipsis_probability', 0.20):
            # å°†æŸä¸ªå¥å·æ›¿æ¢ä¸ºçœç•¥å·
            text = re.sub(r'\.(\s+)', r'...\1', text, count=1)

        # æ„Ÿå¹å·èŠ‚åˆ¶ï¼ˆæœ€å¤š1ä¸ªï¼‰
        if punctuation.get('exclamation_moderation', True):
            exclamation_count = text.count('!')
            max_allowed = punctuation.get('max_exclamations_per_comment', 1)
            if exclamation_count > max_allowed:
                # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œå…¶ä½™æ›¿æ¢ä¸ºå¥å·
                parts = text.split('!')
                text = parts[0] + '!' + '.'.join(parts[1:])

        return text

    def _default_policy(self) -> Dict:
        """é»˜è®¤è‡ªç„¶åŒ–ç­–ç•¥ï¼ˆå½“é…ç½®æ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼‰"""
        return {
            'emoji_policy': {
                'allow': True,
                'probability': 0.25,
                'max_per_comment': 1,
                'appropriate_emojis': ['ğŸ‘', 'ğŸ˜‚', 'ğŸ™', 'ğŸ¤”', 'ğŸ‘€']
            },
            'typo_policy': {
                'allow_light_typos': True,
                'probability': 0.15,
                'max_typo_ratio': 0.03,
                'allowed_typos': [
                    {'original': 'transfer', 'typo': 'tranfer'},
                    {'original': 'receive', 'typo': 'recieve'},
                    {'original': 'definitely', 'typo': 'definately'}
                ]
            },
            'filler_words': {
                'allow': True,
                'probability': 0.35,
                'common_fillers': ['tbh', 'imo', 'fwiw', 'honestly', 'actually'],
                'max_filler_ratio': 0.1
            },
            'punctuation_variety': {
                'allow_ellipsis': True,
                'ellipsis_probability': 0.20,
                'exclamation_moderation': True,
                'max_exclamations_per_comment': 1
            }
        }
