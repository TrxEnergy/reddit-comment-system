"""
M4å†…å®¹å·¥å‚ - è‡ªç„¶åŒ–å¤„ç†å™¨
è´Ÿè´£æ£€æŸ¥å’Œä¼˜åŒ–è¯„è®ºçš„è‡ªç„¶åº¦å’Œå¤šæ ·æ€§
"""

import re
import random
import yaml
from pathlib import Path
from typing import List, Dict, Optional
from collections import Counter

from src.content.models import Persona
from src.core.logging import get_logger

logger = get_logger(__name__)


class Naturalizer:
    """
    è‡ªç„¶åŒ–å¤„ç†å™¨
    æ£€æŸ¥å¥å¼å¤šæ ·æ€§ã€å£å¤´ç¦…å¯†åº¦ã€n-gramé‡å¤åº¦
    [FIX 2025-10-10] æ–°å¢ï¼šå£å¤´ç¦…æ³¨å…¥ã€è‡ªç„¶ç‘•ç–µæ·»åŠ ã€å¥å¼å¤šæ ·åŒ–
    """

    def __init__(
        self,
        ngram_window_days: int = 7,
        max_ngram_reuse: float = 0.35,
        naturalization_policy_path: Optional[Path] = None
    ):
        """
        åˆå§‹åŒ–è‡ªç„¶åŒ–å¤„ç†å™¨

        Args:
            ngram_window_days: n-gramå»é‡çª—å£ï¼ˆå¤©ï¼‰
            max_ngram_reuse: æœ€å¤§n-gramé‡å¤ç‡
            naturalization_policy_path: è‡ªç„¶åŒ–ç­–ç•¥é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.ngram_window_days = ngram_window_days
        self.max_ngram_reuse = max_ngram_reuse
        self.ngram_history: List[str] = []  # å­˜å‚¨å†å²8-gram

        # [FIX 2025-10-10] åŠ è½½è‡ªç„¶åŒ–ç­–ç•¥é…ç½®
        self.policy = {}
        if naturalization_policy_path and naturalization_policy_path.exists():
            with open(naturalization_policy_path, 'r', encoding='utf-8') as f:
                self.policy = yaml.safe_load(f)
        else:
            # é»˜è®¤ç­–ç•¥
            self.policy = self._default_policy()

    def process(
        self,
        comment_text: str,
        persona: Persona
    ) -> str:
        """
        å¤„ç†è¯„è®ºæ–‡æœ¬ï¼Œç¡®ä¿è‡ªç„¶åº¦

        [FIX 2025-10-10] æ–°å¢è‡ªåŠ¨å¤„ç†æ­¥éª¤ï¼š
        1. æ³¨å…¥å£å¤´ç¦…ï¼ˆæ ¹æ®Personaï¼‰
        2. æ·»åŠ è‡ªç„¶ç‘•ç–µï¼ˆè¡¨æƒ…ã€é”™å­—ã€å£è¯­è¯ï¼‰
        3. å¥å¼å¤šæ ·åŒ–
        4. æ£€æŸ¥è´¨é‡ï¼ˆå¥å¼ã€å¯†åº¦ã€n-gramï¼‰

        Args:
            comment_text: åŸå§‹è¯„è®ºæ–‡æœ¬
            persona: ä½¿ç”¨çš„Persona

        Returns:
            å¤„ç†åçš„è‡ªç„¶åŒ–è¯„è®ºæ–‡æœ¬
        """
        # [FIX 2025-10-10] æ­¥éª¤1ï¼šæ³¨å…¥å£å¤´ç¦…
        text = self.inject_catchphrases(comment_text, persona)

        # [FIX 2025-10-10] æ­¥éª¤2ï¼šæ·»åŠ è‡ªç„¶ç‘•ç–µ
        text = self.add_natural_imperfections(text)

        # [FIX 2025-10-10] æ­¥éª¤3ï¼šå¥å¼å¤šæ ·åŒ–
        text = self.vary_sentence_structure(text)

        # åŸæœ‰æ£€æŸ¥æµç¨‹
        sentence_issues = self._check_sentence_variety(text)
        if sentence_issues:
            logger.debug("Sentence variety issues detected", issues=sentence_issues)

        catchphrase_issues = self._check_catchphrase_density(
            text,
            persona.catchphrases
        )
        if catchphrase_issues:
            logger.debug("Catchphrase density issues detected", issues=catchphrase_issues)

        ngram_issues = self._check_ngram_uniqueness(text)
        if ngram_issues:
            logger.warning("N-gram repetition detected", issues=ngram_issues)

        self._record_ngrams(text)

        return text

    def _check_sentence_variety(self, text: str) -> List[str]:
        """
        æ£€æŸ¥å¥å¼å¤šæ ·æ€§

        æ£€æŸ¥ï¼š
        - æ˜¯å¦è‡³å°‘åŒ…å«2ç§å¥å¼ï¼ˆé™ˆè¿°/ç–‘é—®/åˆ—è¡¨ï¼‰
        - å¥å¼ç»“æ„é‡å¤ç‡æ˜¯å¦è¶…è¿‡50%
        """
        issues = []

        sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]
        if len(sentences) < 2:
            return issues  # å¤ªçŸ­ï¼Œæ— éœ€æ£€æŸ¥

        # ç»Ÿè®¡å¥å¼ç±»å‹
        statements = sum(1 for s in sentences if not s.endswith('?'))
        questions = sum(1 for s in sentences if s.endswith('?'))

        sentence_types = sum([statements > 0, questions > 0])
        if sentence_types < 2:
            issues.append("Only one sentence type detected")

        # æ£€æŸ¥å¥å­ç»“æ„é‡å¤ï¼ˆç®€åŒ–ï¼šæ£€æŸ¥å¼€å¤´è¯ï¼‰
        opening_words = [s.split()[0].lower() for s in sentences if s.split()]
        if opening_words:
            most_common_count = Counter(opening_words).most_common(1)[0][1]
            repeat_ratio = most_common_count / len(opening_words)
            if repeat_ratio > 0.5:
                issues.append(f"Sentence structure repetition: {repeat_ratio:.0%}")

        return issues

    def _check_catchphrase_density(
        self,
        text: str,
        catchphrases: Dict[str, List[str]]
    ) -> List[str]:
        """
        æ£€æŸ¥å£å¤´ç¦…å¯†åº¦

        ç›®æ ‡ï¼šåŒpersona 7å¤©å†…å£å¤´ç¦…ä¸è¶…è¿‡20%
        """
        issues = []

        all_phrases = []
        for phrases_list in catchphrases.values():
            all_phrases.extend(phrases_list)

        text_lower = text.lower()
        sentences = [s.strip() for s in re.split(r'[.!?]', text) if s.strip()]

        catchphrase_count = sum(1 for phrase in all_phrases if phrase.lower() in text_lower)
        if not sentences:
            return issues

        density = catchphrase_count / len(sentences)
        if density > 0.20:
            issues.append(f"Catchphrase density too high: {density:.0%} (max 20%)")

        return issues

    def _check_ngram_uniqueness(self, text: str) -> List[str]:
        """
        æ£€æŸ¥8-gramé‡å¤åº¦

        ä¸å†å²è®°å½•å¯¹æ¯”ï¼Œé‡å¤ç‡ä¸è¶…è¿‡35%
        """
        issues = []

        current_ngrams = self._extract_8grams(text)
        if not current_ngrams:
            return issues

        # ä¸å†å²å¯¹æ¯”
        if self.ngram_history:
            overlap = sum(1 for ng in current_ngrams if ng in self.ngram_history)
            reuse_ratio = overlap / len(current_ngrams)

            if reuse_ratio > self.max_ngram_reuse:
                issues.append(
                    f"8-gram reuse rate: {reuse_ratio:.0%} (max {self.max_ngram_reuse:.0%})"
                )

        return issues

    def _extract_8grams(self, text: str) -> List[str]:
        """
        æå–æ–‡æœ¬ä¸­çš„8-gramï¼ˆè¿ç»­8ä¸ªè¯ï¼‰

        Returns:
            8-gramå­—ç¬¦ä¸²åˆ—è¡¨
        """
        words = text.split()
        if len(words) < 8:
            return []

        ngrams = []
        for i in range(len(words) - 7):
            ngram = ' '.join(words[i:i+8])
            ngrams.append(ngram.lower())

        return ngrams

    def _record_ngrams(self, text: str):
        """è®°å½•å½“å‰è¯„è®ºçš„8-gramåˆ°å†å²"""
        ngrams = self._extract_8grams(text)
        self.ngram_history.extend(ngrams)

        # é™åˆ¶å†å²å¤§å°ï¼ˆä¿ç•™æœ€è¿‘1000ä¸ªï¼‰
        if len(self.ngram_history) > 1000:
            self.ngram_history = self.ngram_history[-1000:]

    def clear_history(self):
        """æ¸…ç©ºn-gramå†å²ï¼ˆæµ‹è¯•/è°ƒè¯•ç”¨ï¼‰"""
        self.ngram_history.clear()
        logger.info("Cleared n-gram history")

    # [FIX 2025-10-10] æ–°å¢æ–¹æ³•

    def inject_catchphrases(self, text: str, persona: Persona) -> str:
        """
        éšæœºæ³¨å…¥Personaå£å¤´ç¦…ï¼ˆå¼€å¤´/è¿‡æ¸¡/ç»“å°¾ï¼‰

        ç­–ç•¥ï¼š
        - 25%æ¦‚ç‡åœ¨å¼€å¤´æ³¨å…¥opening catchphrase
        - 20%æ¦‚ç‡åœ¨ä¸­é—´æ³¨å…¥transition catchphrase
        - 30%æ¦‚ç‡åœ¨ç»“å°¾æ³¨å…¥ending catchphrase
        """
        # [FIX 2025-10-10] ä¿®å¤ï¼šä½¿ç”¨æ›´å¥½çš„å¥å­åˆ†å‰²ï¼Œä¿ç•™æ ‡ç‚¹
        sentences = re.split(r'(?<=[.!?])\s+', text)
        if len(sentences) < 2:
            return text  # å¤ªçŸ­ï¼Œä¸æ³¨å…¥

        # å¼€å¤´æ³¨å…¥ï¼ˆ25%æ¦‚ç‡ï¼‰
        if random.random() < 0.25 and persona.catchphrases.get('opening'):
            opening_phrase = random.choice(persona.catchphrases['opening'])
            # å¦‚æœç¬¬ä¸€å¥ä¸æ˜¯ä»¥å£å¤´ç¦…å¼€å¤´ï¼Œåˆ™æ·»åŠ 
            if not any(sentences[0].lower().startswith(p.lower()) for p in persona.catchphrases.get('opening', [])):
                sentences[0] = f"{opening_phrase} {sentences[0]}"

        # è¿‡æ¸¡æ³¨å…¥ï¼ˆ20%æ¦‚ç‡ï¼Œæ’å…¥ä¸­é—´ï¼‰
        if random.random() < 0.20 and persona.catchphrases.get('transition'):
            if len(sentences) >= 3:  # è‡³å°‘3å¥æ‰æ’å…¥ä¸­é—´
                transition_phrase = random.choice(persona.catchphrases['transition'])
                mid_idx = len(sentences) // 2
                sentences[mid_idx] = f"{transition_phrase} {sentences[mid_idx]}"

        # ç»“å°¾æ³¨å…¥ï¼ˆ30%æ¦‚ç‡ï¼‰
        if random.random() < 0.30 and persona.catchphrases.get('ending'):
            ending_phrase = random.choice(persona.catchphrases['ending'])
            # åœ¨æœ€åæ·»åŠ 
            if not sentences[-1].endswith('.'):
                sentences[-1] += '.'
            sentences[-1] = sentences[-1].rstrip('.') + f'. {ending_phrase}'

        return ' '.join(sentences)

    def add_natural_imperfections(self, text: str) -> str:
        """
        æ·»åŠ è‡ªç„¶ç‘•ç–µï¼ˆé«˜åº¦æ‹ŸäººåŒ–ç‰ˆæœ¬ 2025-10-15ï¼‰

        è°ƒç”¨é¡ºåº:
        1. å¤§å°å†™æ‹ŸäººåŒ–ï¼ˆå¥é¦–å°å†™ã€å…¨å¤§å†™å¼ºè°ƒã€ä¸“æœ‰åè¯éšæ„åŒ–ï¼‰
        2. åŠ å¯†ä¿šè¯­æ³¨å…¥ï¼ˆHODLã€rektç­‰ï¼‰
        3. é”™åˆ«å­—ï¼ˆ25%æ¦‚ç‡ï¼‰
        4. æ’‡å·çœç•¥ï¼ˆ20%æ¦‚ç‡ï¼‰
        5. å£è¯­å¡«å……è¯ï¼ˆ45%æ¦‚ç‡ï¼‰
        6. å¤šemojiï¼ˆ45%æ¦‚ç‡ï¼Œæœ€å¤š2ä¸ªï¼‰
        7. æ ‡ç‚¹éšæ„åŒ–ï¼ˆçœç•¥å¥å·ã€å¤šæ„Ÿå¹å·ç­‰ï¼‰
        """
        text = self.apply_capitalization_humanization(text)

        text = self.inject_crypto_slang(text)

        # [FIX 2025-10-15] é”™åˆ«å­—æ³¨å…¥ - åªé€‰æ‹©æ–‡æœ¬ä¸­å­˜åœ¨çš„è¯
        typo_policy = self.policy.get('typo_policy', {})
        if typo_policy.get('allow_light_typos', True) and random.random() < typo_policy.get('probability', 0.50):
            allowed_typos = typo_policy.get('allowed_typos', [])
            if allowed_typos:
                # ç­›é€‰å‡ºæ–‡æœ¬ä¸­å­˜åœ¨çš„è¯
                available_typos = [
                    t for t in allowed_typos
                    if re.search(r'\b' + t.get('original', '') + r'\b', text, flags=re.IGNORECASE)
                ]

                if available_typos:
                    typo_pair = random.choice(available_typos)
                    original = typo_pair.get('original', '')
                    typo = typo_pair.get('typo', '')
                    if original and typo:
                        text = re.sub(r'\b' + original + r'\b', typo, text, count=1, flags=re.IGNORECASE)

        text = self.drop_apostrophes(text)

        filler_policy = self.policy.get('filler_words', {})
        if filler_policy.get('allow', True) and random.random() < filler_policy.get('probability', 0.48):
            fillers = filler_policy.get('common_fillers', ['tbh', 'imo', 'honestly'])
            filler = random.choice(fillers)
            sentences = re.split(r'([.!?])', text, maxsplit=1)
            if sentences:
                sentences[0] = f"{filler}, {sentences[0]}"
                text = ''.join(sentences)

        text = self.apply_multi_emoji(text)

        # [FIX 2025-10-15] çœç•¥å·æ³¨å…¥ï¼ˆ35%æ¦‚ç‡ï¼‰- ä»vary_sentence_structure()ç§»åˆ°è¿™é‡Œ
        punct_policy = self.policy.get('punctuation_variety', {})
        if punct_policy.get('allow_ellipsis', True) and random.random() < punct_policy.get('ellipsis_probability', 0.35):
            ellipsis_types = punct_policy.get('ellipsis_types', ['...', '..', 'â€¦'])
            ellipsis = random.choice(ellipsis_types)
            # å°†æŸä¸ªå¥å·æ›¿æ¢ä¸ºçœç•¥å·
            text = re.sub(r'\.(\s+)', ellipsis + r'\1', text, count=1)

        text = self.apply_punctuation_casualization(text)

        return text

    def vary_sentence_structure(self, text: str) -> str:
        """
        å¥å¼å¤šæ ·åŒ–ï¼ˆè½»é‡å®ç°ï¼‰

        ç­–ç•¥ï¼š
        - æ·»åŠ çœç•¥å·ï¼ˆ20%æ¦‚ç‡ï¼‰
        - è°ƒæ•´æ ‡ç‚¹ï¼ˆé¿å…è¿‡å¤šæ„Ÿå¹å·ï¼‰
        """
        punctuation = self.policy.get('punctuation_variety', {})

        # çœç•¥å·æ›¿æ¢ï¼ˆ20%æ¦‚ç‡ï¼‰
        if punctuation.get('allow_ellipsis', True) and random.random() < punctuation.get('ellipsis_probability', 0.20):
            # å°†æŸä¸ªå¥å·æ›¿æ¢ä¸ºçœç•¥å·
            text = re.sub(r'\.(\s+)', r'...\1', text, count=1)

        # æ„Ÿå¹å·èŠ‚åˆ¶ï¼ˆæœ€å¤š1ä¸ªï¼‰
        if punctuation.get('exclamation_moderation', True):
            exclamation_count = text.count('!')
            max_allowed = punctuation.get('max_exclamations_per_comment', 1)
            if exclamation_count > max_allowed:
                # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œå…¶ä½™æ›¿æ¢ä¸ºå¥å·
                parts = text.split('!')
                text = parts[0] + '!' + '.'.join(parts[1:])

        return text

    def _default_policy(self) -> Dict:
        """é»˜è®¤è‡ªç„¶åŒ–ç­–ç•¥ï¼ˆå½“é…ç½®æ–‡ä»¶ä¸å­˜åœ¨æ—¶ï¼‰"""
        return {
            'emoji_policy': {
                'allow': True,
                'probability': 0.25,
                'max_per_comment': 1,
                'appropriate_emojis': ['ğŸ‘', 'ğŸ˜‚', 'ğŸ™', 'ğŸ¤”', 'ğŸ‘€']
            },
            'typo_policy': {
                'allow_light_typos': True,
                'probability': 0.15,
                'max_typo_ratio': 0.03,
                'allowed_typos': [
                    {'original': 'transfer', 'typo': 'tranfer'},
                    {'original': 'receive', 'typo': 'recieve'},
                    {'original': 'definitely', 'typo': 'definately'}
                ]
            },
            'filler_words': {
                'allow': True,
                'probability': 0.48,
                'common_fillers': ['tbh', 'imo', 'fwiw', 'honestly', 'actually'],
                'max_filler_ratio': 0.1
            },
            'punctuation_variety': {
                'allow_ellipsis': True,
                'ellipsis_probability': 0.20,
                'exclamation_moderation': True,
                'max_exclamations_per_comment': 1
            }
        }

    def apply_capitalization_humanization(self, text: str) -> str:
        """
        åº”ç”¨å¤§å°å†™æ‹ŸäººåŒ–

        ç­–ç•¥:
        1. å¥é¦–å°å†™ï¼ˆ30%æ¦‚ç‡ï¼Œç‰¹å®šè§¦å‘è¯æ›´é«˜ï¼‰
        2. å…¨å¤§å†™å¼ºè°ƒï¼ˆ12%æ¦‚ç‡ï¼ŒåŠ å¯†ç¤¾åŒºæ¢—ï¼‰
        3. ä¸“æœ‰åè¯éšæ„åŒ–ï¼ˆ35%å°å†™ï¼‰
        """
        cap_policy = self.policy.get('capitalization_policy', {})

        if not cap_policy.get('allow_lowercase_start', False):
            return text

        sentences = re.split(r'([.!?]\s+)', text)

        for i in range(0, len(sentences), 2):
            if not sentences[i].strip():
                continue

            first_word = sentences[i].split()[0] if sentences[i].split() else ""

            if first_word.lower() in cap_policy.get('lowercase_triggers', []):
                if random.random() < 0.70:
                    sentences[i] = sentences[i][0].lower() + sentences[i][1:] if sentences[i] else sentences[i]
            elif random.random() < cap_policy.get('lowercase_start_probability', 0.30):
                sentences[i] = sentences[i][0].lower() + sentences[i][1:] if sentences[i] else sentences[i]

        text = ''.join(sentences)

        # [FIX 2025-10-15] å…¨å¤§å†™å¼ºè°ƒ - åªé€‰æ‹©æ–‡æœ¬ä¸­å­˜åœ¨çš„è¯
        if cap_policy.get('allow_all_caps_emphasis', False) and random.random() < cap_policy.get('all_caps_probability', 0.30):
            all_caps_words = cap_policy.get('all_caps_whitelist', [])
            if all_caps_words:
                # ç­›é€‰å‡ºæ–‡æœ¬ä¸­å­˜åœ¨çš„è¯
                available_words = [w for w in all_caps_words if re.search(r'\b' + w.lower() + r'\b', text, flags=re.IGNORECASE)]
                if available_words:
                    word_to_caps = random.choice(available_words)
                    text = re.sub(r'\b' + word_to_caps.lower() + r'\b', word_to_caps, text, count=1, flags=re.IGNORECASE)

        if not cap_policy.get('proper_noun_strict', True):
            casual_nouns = cap_policy.get('casual_lowercase_nouns', [])
            lowercase_prob = cap_policy.get('proper_noun_lowercase_probability', 0.35)
            for noun in casual_nouns:
                if random.random() < lowercase_prob:
                    text = re.sub(r'\b' + noun.capitalize() + r'\b', noun, text)

        return text

    def apply_punctuation_casualization(self, text: str) -> str:
        """
        åº”ç”¨æ ‡ç‚¹éšæ„åŒ–

        ç­–ç•¥:
        1. çœç•¥ç»“å°¾å¥å·ï¼ˆ40%æ¦‚ç‡ï¼‰
        2. çœç•¥é€—å·ï¼ˆ15%æ¦‚ç‡ï¼‰
        3. å¤šæ„Ÿå¹å·ï¼ˆ25%æ¦‚ç‡ï¼‰
        4. ä¿®è¾é—®å¥ï¼ˆ15%æ¦‚ç‡ï¼‰
        """
        punct_policy = self.policy.get('punctuation_variety', {})

        if punct_policy.get('allow_missing_period', False):
            conditions = punct_policy.get('missing_period_conditions', {})
            word_count = len(text.split())
            ends_with_emoji = bool(re.search(r'[ğŸ˜€-ğŸ™ğŸ’€-ğŸŸ¿]$', text))

            should_drop = False
            if conditions.get('short_comment') and word_count < 15:
                should_drop = random.random() < punct_policy.get('missing_period_probability', 0.40)
            elif conditions.get('ends_with_emoji') and ends_with_emoji:
                should_drop = random.random() < 0.60

            if should_drop and text.endswith('.'):
                text = text.rstrip('.')

        if punct_policy.get('allow_missing_comma', False):
            missing_comma_contexts = punct_policy.get('missing_comma_contexts', [])
            for context in missing_comma_contexts:
                if random.random() < punct_policy.get('missing_comma_probability', 0.15):
                    text = re.sub(r'\b' + context + r',\s+', context + ' ', text, count=1)

        if punct_policy.get('allow_multiple_exclamations', False):
            exclamation_patterns = punct_policy.get('exclamation_patterns', ['!'])
            if '!' in text and random.random() < 0.25:
                pattern = random.choice(exclamation_patterns)
                text = re.sub(r'!+', pattern, text, count=1)

        return text

    def drop_apostrophes(self, text: str) -> str:
        """
        çœç•¥ç¼©å†™æ’‡å·ï¼ˆ50%æ¦‚ç‡ï¼‰

        ç¤ºä¾‹: don't â†’ dont, can't â†’ cant
        [FIX 2025-10-15] åªé€‰æ‹©æ–‡æœ¬ä¸­å­˜åœ¨çš„ç¼©å†™
        """
        contraction_policy = self.policy.get('contraction_policy', {})

        if not contraction_policy.get('allow_apostrophe_drop', False):
            return text

        if random.random() < contraction_policy.get('apostrophe_drop_probability', 0.50):
            casual_contractions = contraction_policy.get('casual_contractions', [])
            if casual_contractions:
                # ç­›é€‰å‡ºæ–‡æœ¬ä¸­å­˜åœ¨çš„ç¼©å†™
                available_contractions = [
                    c for c in casual_contractions
                    if c.get('original', '') in text
                ]

                if available_contractions:
                    contraction = random.choice(available_contractions)
                    original = contraction.get('original', '')
                    casual = contraction.get('casual', '')
                    if original and casual:
                        text = text.replace(original, casual, 1)

        return text

    def inject_crypto_slang(self, text: str) -> str:
        """
        æ³¨å…¥åŠ å¯†ç¤¾åŒºä¿šè¯­ï¼ˆ50%æ¦‚ç‡ï¼‰

        ç¤ºä¾‹: hold â†’ HODL, destroyed â†’ rekt
        [FIX 2025-10-15] åªé€‰æ‹©æ–‡æœ¬ä¸­å­˜åœ¨çš„è¯è¿›è¡Œæ›¿æ¢
        """
        slang_policy = self.policy.get('crypto_slang', {})

        if not slang_policy.get('allow', False):
            return text

        if random.random() < slang_policy.get('slang_probability', 0.50):
            replacements = slang_policy.get('replacements', [])
            if replacements:
                # ç­›é€‰å‡ºæ–‡æœ¬ä¸­å­˜åœ¨çš„formalè¯
                available_replacements = [
                    r for r in replacements
                    if re.search(r'\b' + r.get('formal', '') + r'\b', text, flags=re.IGNORECASE)
                ]

                if available_replacements:
                    replacement = random.choice(available_replacements)
                    formal = replacement.get('formal', '')
                    slang = replacement.get('slang', '')
                    prob = replacement.get('probability', 1.0)

                    if formal and slang and random.random() < prob:
                        text = re.sub(r'\b' + formal + r'\b', slang, text, count=1, flags=re.IGNORECASE)

        return text

    def apply_multi_emoji(self, text: str) -> str:
        """
        åº”ç”¨å¤šemojiç­–ç•¥ï¼ˆ45%æ¦‚ç‡ï¼Œæœ€å¤š2ä¸ªï¼‰

        ç­–ç•¥:
        1. ä¸Šä¸‹æ–‡åŒ¹é…emoji
        2. å¤šä½ç½®æ”¾ç½®ï¼ˆç»“å°¾ã€å¥ä¸­ã€å¼€å¤´ï¼‰
        """
        emoji_policy = self.policy.get('emoji_policy', {})

        if not emoji_policy.get('allow', True):
            return text

        max_emojis = emoji_policy.get('max_per_comment', 2)
        probability = emoji_policy.get('probability', 0.45)

        if random.random() > probability:
            return text

        emojis_to_add = random.randint(1, max_emojis)
        emojis = emoji_policy.get('appropriate_emojis', ['ğŸ‘', 'ğŸ˜‚', 'ğŸ™'])
        placements = emoji_policy.get('placement', ['ending'])

        for _ in range(emojis_to_add):
            if not emojis:
                break

            emoji = random.choice(emojis)
            placement = random.choice(placements)

            if placement == 'ending':
                text = text.rstrip() + f" {emoji}"
            elif placement == 'mid_sentence' and '. ' in text:
                parts = text.split('. ', 1)
                text = f"{parts[0]} {emoji}. {parts[1]}"
            elif placement == 'opening':
                text = f"{emoji} {text}"

        return text
